{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision #provide access to datasets, models, transforms, utils, etc\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring constants for epoch and batch size\n",
    "EPOCH = 20\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for creating custom dataset in required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating MNIST dataset from inbuilt datasets\n",
    "mnist_train_set = torchvision.datasets.MNIST(\n",
    "            root='./try1',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11,  2, 11,  ..., 13, 10, 12])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying the sum of 60000 random integers with labels of train dataset of mnist\n",
    "sum_labels = mnist_train_set.targets + torch.randint(0, 9, (1, 60000)).squeeze()\n",
    "sum_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accessing a sample from dataset \n",
    "sample = next(iter(mnist_train_set))\n",
    "image, label = sample\n",
    "image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visulaizing the sample from dataset\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "print('label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_train_set.data) # checking the length of dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating random integers between 0 to 9 similar to size of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rn = torch.randint(0, 9, (1, 60000)).squeeze() \n",
    "rn[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting labels for addtion into their respective probability distribution\n",
    "\n",
    "This is necessory for feeding it to Cross Entropy Loss function for calculating loss, as it expects the tensors containing the values ranging between 0 and 1. \n",
    "- first we convert the sum into their respective binary represenation\n",
    "- dividing it by count of ones to find the respective probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3333, 0.0000, 0.3333, 0.3333, 0.0000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_pd(num):\n",
    "    binary = torch.zeros((5,), dtype=torch.float32) #creating tensor of size five containing only zeros for binary \n",
    "    i = 0\n",
    "    # converting the number to binary\n",
    "    while(num != 0):\n",
    "        if num % 2 == 1:\n",
    "            binary[i] = 1.0\n",
    "        num = torch.div(num, 2, rounding_mode='trunc')\n",
    "        i +=1\n",
    "    ones = (binary == 1.0).sum(dim=0) # counting the number of ones to use it to divide the binary tensor\n",
    "    #we will be having maximum one count = 4, as the maximum value we will have is 18\n",
    "    if ones == 2: # if number of ones = two, prob = 0.5\n",
    "        pd = binary / 2.0 \n",
    "    if ones == 3: # if number of ones = two, prob = 0.33 \n",
    "        pd = binary / 3.0\n",
    "    if ones == 4: # if number of ones = two, prob = 0.25\n",
    "        pd = binary / 4.0\n",
    "\n",
    "    return pd\n",
    "\n",
    "to_pd(13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our required Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTWithNumbers(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # creating dataset for MNIST images\n",
    "        self.mnist_data = torchvision.datasets.MNIST(\n",
    "            root='./data',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "        )\n",
    "        # creating data for random integers between 0 and 9 with the size of MNIST data\n",
    "        self.random_numbers = torch.randint(0, 9, (1, len(self.mnist_data.targets))).squeeze()\n",
    "        # One Hot Encoding the random numbers to be used for NN\n",
    "        self.numbers = torch.zeros((len(self.mnist_data.targets), 10))\n",
    "        for i in range(len(self.mnist_data.targets)):\n",
    "            self.numbers[i][self.random_numbers[i]] = 1\n",
    "\n",
    "    # implementing __getitem__ method \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.mnist_data[index]\n",
    "        image = sample[0]\n",
    "        label = torch.tensor(sample[1])\n",
    "        number = self.numbers[index]\n",
    "        sum_ = label + self.random_numbers[index]\n",
    "        sum_label = to_pd(sum_)\n",
    "        # returing the image, label of image, one-hot encoded number, and probability distribution of sums\n",
    "        return image, label, number, sum_label \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_data.data) # returning the size of data for iterating\n",
    "    \n",
    "    @property #  property for accessing the labels\n",
    "    def train_labels(self): \n",
    "        sum_labels = self.mnist_data.targets + self.random_numbers\n",
    "        return self.mnist_data.targets, sum_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_with_numbers = MNISTWithNumbers() # creating dataset object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000]), torch.Size([60000]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets, sum_labels = mnist_with_numbers.train_labels # checking the size of labels for both the predictions\n",
    "targets.shape, sum_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), torch.Size([]), torch.Size([10]), torch.Size([5]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(mnist_with_numbers)) # checking if we are able to access the data and labels\n",
    "image, label, number, sum_label = sample\n",
    "image.shape, label.shape, number.shape, sum_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0., 0., 0., 1., 0.]),\n",
       " tensor(5))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number, sum_label, label # checking if the data is created in the required manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing dataset to Data Loader to feed NN in batches\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    mnist_with_numbers,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 28, 28]),\n",
       " torch.Size([128]),\n",
       " torch.Size([128, 10]),\n",
       " torch.Size([128, 5]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_data_loader))\n",
    "images, labels, numbers, sum_labels = batch\n",
    "images.shape, labels.shape, numbers.shape, sum_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        # defining the required layers for building the NN architecture\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3) \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(in_features=1600, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=130, out_features=60)\n",
    "        self.out1 = nn.Linear(in_features=60, out_features=10)\n",
    "        self.fc3 = nn.Linear(in_features=60, out_features=30)\n",
    "        self.out2 = nn.Linear(in_features=30, out_features=5)\n",
    "\n",
    "    def forward(self, t1, t2):\n",
    "        # input layer\n",
    "        x1 = t1\n",
    "        x2 = t2\n",
    "\n",
    "        # conv1 layer\n",
    "        x1 = self.conv1(x1) # 28 | 26\n",
    "        x1 = F.relu(x1)\n",
    "\n",
    "        # conv2 layer\n",
    "        x1 = self.conv2(x1) # 26 | 24\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = F.max_pool2d(x1, kernel_size=2, stride=2) # 24 | 12\n",
    "        \n",
    "        # conv3 layer\n",
    "        x1 = self.conv3(x1) # 12 | 10\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = F.max_pool2d(x1, kernel_size=2, stride=2) # 10 | 5\n",
    "\n",
    "        # flattening the tensor till dimension 1 and keeping the batches\n",
    "        x1 = x1.flatten(1) # 64 * 5 * 5 = 1600\n",
    "\n",
    "        # fc1 layer\n",
    "        x1 = self.fc1(x1) # 1600 | 120\n",
    "        x1 = F.relu(x1)\n",
    "\n",
    "        # concatenating the random number tensor with output from first fully connected layer\n",
    "        # combined the inputs here as this layer has only 120 neurons and adding 10 more will \n",
    "        # be significant in number than the previous layer as it has 1600 neurons, if the random \n",
    "        # number was combined in last layer the chances of ignoring it by NN would have been higher, \n",
    "        # but here in the current layer of 120 neurons adding it would give the needed attention \n",
    "        # by NN for addition with label of MNIST\n",
    "        x = torch.cat((x1, x2), 1) # 120 + 10 \n",
    "\n",
    "        # fc2 layer \n",
    "        x = self.fc2(x) # 130 | 60\n",
    "        x = F.relu(x)\n",
    "\n",
    "        #  out1 layer\n",
    "        out1 = self.out1(x) # 60 | 10\n",
    "        out1 = F.softmax(out1, dim=1)\n",
    "\n",
    "        # branching out fully connected hidden layer from the last hidden layer (for first output)  \n",
    "        # for addition of MNIST label and random number\n",
    "        # fc3 layer\n",
    "        out2 = self.fc3(x) # 60 | 30\n",
    "        out2 = F.relu(out2)\n",
    "\n",
    "        # out2 layer\n",
    "\n",
    "        out2 = self.out2(out2) # 30 | 5\n",
    "        out2 = F.softmax(out2, dim=1)\n",
    "\n",
    "        return out1, out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0875, 0.0937, 0.1133, 0.0961, 0.1123, 0.0908, 0.0917, 0.1098, 0.1124,\n",
       "          0.0925]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.1977, 0.2441, 0.1743, 0.2130, 0.1709]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking whether NN is giving the required output\n",
    "out1, out2 = network(image.unsqueeze(0), number.unsqueeze(0))\n",
    "out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1990, 0.2419, 0.1729, 0.2139, 0.1723], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 0., 1., 0.], grad_fn=<SelectBackward0>),\n",
       " tensor([1., 1., 1., 0., 0.]),\n",
       " tensor(False))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# devising the strategy for evaluating the accuracy for addition\n",
    "sum_labels[sum_labels > 0] = 1\n",
    "sum_labels\n",
    "\n",
    "out1, out2 = network(images, numbers)\n",
    "n = sum_labels[0].eq(1.).sum().item()\n",
    "_, indices = torch.topk(out2[0], n)\n",
    "print(out2[0])\n",
    "out2[0][indices] = 1.\n",
    "out2[0][out2[0] < 1] = 0.\n",
    "out2[0], sum_labels[0], torch.all(out2[0].eq(sum_labels[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation function for calculating accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_correct(preds, labels): # for calculating the total correct prediction made for MNIST images\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "def get_preds_sum_correct(preds, labels, batch_size): # for calculating the total correct prediction for addition \n",
    "    count = 0\n",
    "    labels[labels > 0] = 1. # converting the probabilites back into ones (binary represntation)\n",
    "    for i in range(batch_size):\n",
    "        n =  labels[i].eq(1.).sum().item() # n = number of ones in the binary\n",
    "        if n > 0:\n",
    "            _, index = torch.topk(preds[i], n) # taking the indices for top n values in prediction to match against the label\n",
    "            preds[i][index] = 1.                # converting the top n values into 1 \n",
    "    preds[preds<1] = 0.\n",
    "    for i in range(batch_size):\n",
    "        if torch.all(labels[i].eq(preds[i])):   # checking if the predictions are equal to labels \n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 9)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_preds_correct(out1, labels), get_preds_sum_correct(out2, sum_labels, 32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing if everything is working fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 2.301152229309082 1.583652377128601\n",
      "loss2: 2.300562858581543 1.5824871063232422\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "mnist_with_numbers = MNISTWithNumbers()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    mnist_with_numbers,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "network = Network()\n",
    "# optimizer for performing weight updates using Adam optimizer\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "batch = next(iter(train_loader)) #get batch\n",
    "images, labels, numbers, sum_labels = batch\n",
    "pred_labels, pred_sums = network(images, numbers)\n",
    "loss_image = F.cross_entropy(pred_labels, labels) # calculating loss for image\n",
    "loss_sum = F.cross_entropy(pred_sums, sum_labels) # calculating loss for sum\n",
    "\n",
    "loss_image.backward(retain_graph=True) #calculate gradients and save the graph for update by the loss for additon\n",
    "loss_sum.backward() # calculate the gradients for the addition\n",
    "optimizer.step() #update weights\n",
    "\n",
    "print('loss1:', loss_image.item(), loss_sum.item())\n",
    "pred_labels, pred_sums = network(images, numbers)\n",
    "loss_image = F.cross_entropy(pred_labels, labels) \n",
    "loss_sum = F.cross_entropy(pred_sums, sum_labels)\n",
    "print('loss2:',loss_image.item(), loss_sum.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# checking if the GPU is available, and if yes, switching the process to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 \n",
      " \t MNIST: total loss:  841.63 acc:  66.52 \n",
      " \t Addition: total loss:  716.23 acc:  30.84\n",
      "epoch: 1 \n",
      " \t MNIST: total loss:  742.74 acc:  87.79 \n",
      " \t Addition: total loss:  706.50 acc:  34.83\n",
      "epoch: 2 \n",
      " \t MNIST: total loss:  714.16 acc:  94.09 \n",
      " \t Addition: total loss:  666.46 acc:  43.20\n",
      "epoch: 3 \n",
      " \t MNIST: total loss:  693.29 acc:  98.48 \n",
      " \t Addition: total loss:  640.79 acc:  49.05\n",
      "epoch: 4 \n",
      " \t MNIST: total loss:  691.48 acc:  98.83 \n",
      " \t Addition: total loss:  633.88 acc:  50.19\n",
      "epoch: 5 \n",
      " \t MNIST: total loss:  690.57 acc:  98.96 \n",
      " \t Addition: total loss:  631.04 acc:  51.63\n",
      "epoch: 6 \n",
      " \t MNIST: total loss:  689.55 acc:  99.17 \n",
      " \t Addition: total loss:  628.63 acc:  54.28\n",
      "epoch: 7 \n",
      " \t MNIST: total loss:  688.89 acc:  99.29 \n",
      " \t Addition: total loss:  627.46 acc:  54.55\n",
      "epoch: 8 \n",
      " \t MNIST: total loss:  688.53 acc:  99.37 \n",
      " \t Addition: total loss:  626.80 acc:  56.18\n",
      "epoch: 9 \n",
      " \t MNIST: total loss:  688.17 acc:  99.43 \n",
      " \t Addition: total loss:  626.05 acc:  57.03\n",
      "epoch: 10 \n",
      " \t MNIST: total loss:  687.93 acc:  99.48 \n",
      " \t Addition: total loss:  625.39 acc:  58.13\n",
      "epoch: 11 \n",
      " \t MNIST: total loss:  687.58 acc:  99.56 \n",
      " \t Addition: total loss:  624.88 acc:  58.42\n",
      "epoch: 12 \n",
      " \t MNIST: total loss:  687.59 acc:  99.54 \n",
      " \t Addition: total loss:  624.77 acc:  58.55\n",
      "epoch: 13 \n",
      " \t MNIST: total loss:  687.40 acc:  99.59 \n",
      " \t Addition: total loss:  624.50 acc:  58.54\n",
      "epoch: 14 \n",
      " \t MNIST: total loss:  687.06 acc:  99.67 \n",
      " \t Addition: total loss:  623.30 acc:  58.62\n",
      "epoch: 15 \n",
      " \t MNIST: total loss:  686.86 acc:  99.71 \n",
      " \t Addition: total loss:  622.24 acc:  60.66\n",
      "epoch: 16 \n",
      " \t MNIST: total loss:  686.50 acc:  99.77 \n",
      " \t Addition: total loss:  621.23 acc:  62.18\n",
      "epoch: 17 \n",
      " \t MNIST: total loss:  686.77 acc:  99.72 \n",
      " \t Addition: total loss:  620.66 acc:  62.39\n",
      "epoch: 18 \n",
      " \t MNIST: total loss:  686.58 acc:  99.76 \n",
      " \t Addition: total loss:  620.01 acc:  62.52\n",
      "epoch: 19 \n",
      " \t MNIST: total loss:  686.44 acc:  99.78 \n",
      " \t Addition: total loss:  619.78 acc:  62.64\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "# shifting the parameters of the model to GPU\n",
    "network.to(device)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    \n",
    "    total_correct_MNIST = 0\n",
    "    total_correct_sum = 0\n",
    "    total_loss_MNIST = 0\n",
    "    total_loss_sum = 0\n",
    "    total_size = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        images, labels, numbers, sum_labels = batch\n",
    "        images, labels, numbers, sum_labels = images.to(device), labels.to(device), numbers.to(device), sum_labels.to(device)\n",
    "        pred_images, pred_sums = network(images, numbers)\n",
    "        loss_image = F.cross_entropy(pred_images, labels) # calculating loss for image\n",
    "        loss_sum = F.cross_entropy(pred_sums, sum_labels) # calculating loss for addition\n",
    "\n",
    "        optimizer.zero_grad() # set the grads to zero \n",
    "        loss_image.backward(retain_graph=True)\n",
    "        loss_sum.backward()\n",
    "        optimizer.step() # update weights\n",
    "\n",
    "        # calculating total loss and accuracy for each epoch\n",
    "        total_loss_MNIST += loss_image.item()\n",
    "        total_correct_MNIST += get_preds_correct(pred_images, labels)\n",
    "        total_loss_sum += loss_sum.item()\n",
    "        total_correct_sum += get_preds_sum_correct(pred_sums, sum_labels, len(sum_labels))\n",
    "        total_size += len(images)\n",
    "\n",
    "    print(f'epoch: {epoch} \\n \\t MNIST: total loss: {total_loss_MNIST: .2f} acc: {total_correct_MNIST/total_size*100: .2f} \\n \\t Addition: total loss: {total_loss_sum: .2f} acc: {total_correct_sum/total_size*100: .2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we have achieved the accuracy of 99.78 % for MNIST images and 62.64 % accuracy for addition. \n",
    "- The accuracy for the MNIST images is calculted by equating the argmax of prediction and label of the image\n",
    "- The accuracy for the addition is calculated by converting the prediction into kind of binary representation by taking the top n values, (where n is the number of ones present in labels) and equating it with the labels (binary represantation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48c2112a8328d33382c63d86ada5a19139318595a90575a431b9665a806e2427"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
